{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network with one hidden layer\n",
    "In this notebook I will implement a neural network with one hidden layer. This NN will then be used to classify the digital figures from [MNIST database](http://yann.lecun.com/exdb/mnist/). The output layer would be activated using \"softmax\" since the output is not just 0 or 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set dimensions: (60000, 28, 28)\n",
      "training label dimensions: (60000,)\n",
      "test set dimensions: (10000, 28, 28)\n",
      "test label dimensions: (10000,)\n"
     ]
    }
   ],
   "source": [
    "# data dims\n",
    "print('training set dimensions: ' + str(x_train.shape))\n",
    "print('training label dimensions: ' + str(y_train.shape))\n",
    "print('test set dimensions: ' + str(x_test.shape))\n",
    "print('test label dimensions: ' + str(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 helper functions\n",
    "- sigmoid\n",
    "- relu\n",
    "- softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x)), x\n",
    "def relu(x):\n",
    "    return np.maximum(0, x), x\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x)), x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x182be3a1f08>]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test helper functions\n",
    "x = np.linspace(-10, 10)\n",
    "plt.plot(x, softmax(x)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims):\n",
    "    \"\"\"\n",
    "    Initialize the parameters W's and b's.\n",
    "    \n",
    "    Argument:\n",
    "    layer_dims -- an array of layer dimensions [n0, n1, ..., nL]\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- a dictionary of initialized parameters\n",
    "                    W1: random\n",
    "                    b1: zeros\n",
    "                    ...\n",
    "                    WL: random\n",
    "                    bL: zeros\n",
    "    \"\"\"\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)\n",
    "    for l in range(1, L):\n",
    "        parameters['W'+str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n",
    "        parameters['b'+str(l)] = np.zeros((layer_dims[l], 1))\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'W1': array([[ 0.70489977,  2.02850029,  0.34440333,  1.11052513, -1.08081652],\n",
      "       [ 1.21532409, -1.5404018 ,  1.55449119, -1.18478207, -0.25581854],\n",
      "       [-1.44603519,  0.16279023, -0.52684983, -0.93217436, -1.31376191],\n",
      "       [ 0.65892708,  0.3836431 , -1.49557389,  0.37448568,  1.58956988]]), 'b1': array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]]), 'W2': array([[ 1.70136281,  2.4908491 , -0.00452855,  1.52846754]]), 'b2': array([[0.]])}\n"
     ]
    }
   ],
   "source": [
    "# test initialize_parameters()\n",
    "layer_dims = [5, 4, 1]\n",
    "parameters = initialize_parameters(layer_dims)\n",
    "print(parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear forward\n",
    "def linear_forward(A_prev, W, b):\n",
    "    \"\"\"\n",
    "    The linear part of forward propagation: Z = WA + b\n",
    "    \n",
    "    Argument:\n",
    "    A_prev -- the activitions of previous step (X for the first hidden layer)\n",
    "    W, b -- parameters \n",
    "    \n",
    "    Returns:\n",
    "    Z -- result of linear forward propagation\n",
    "    cache -- cache A_prev and the parameters W and b for convenience when calculating backpropagation\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = W @ A_prev + b\n",
    "    cache = (A_prev, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z: [[ 0.07496603]\n",
      " [-1.29750846]\n",
      " [-6.51086663]\n",
      " [14.86836287]]\n",
      "cache: (array([[1],\n",
      "       [2],\n",
      "       [3],\n",
      "       [4],\n",
      "       [5]]), array([[ 0.24231137,  0.79358088,  0.64267379, -1.6697503 ,  0.59929455],\n",
      "       [ 0.68962104, -0.73433347,  0.65714267,  0.55391356, -0.94110896],\n",
      "       [-1.43654678, -0.07274414, -2.67653623,  0.34781203,  0.34190581],\n",
      "       [-1.09771533, -1.82268597,  1.91273632,  0.89321228,  2.06007841]]), array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]]))\n"
     ]
    }
   ],
   "source": [
    "# test linear forward\n",
    "layer_dims = [5, 4, 1]\n",
    "parameters = initialize_parameters(layer_dims)\n",
    "A_prev = np.array([1, 2, 3, 4, 5]).reshape((5, 1))\n",
    "W = parameters['W1']\n",
    "b = parameters['b1']\n",
    "Z, cache = linear_forward(A_prev, W, b)\n",
    "print('Z: ' + str(Z))\n",
    "print('cache: ' + str(cache))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Forward propagation including both linear mapping and activation\n",
    "    \n",
    "    Argument:\n",
    "    A_prev -- the activations of previous step (X for the first hidden layer)\n",
    "    W, b -- parameters\n",
    "    activation -- activation function used in this layer\n",
    "    \n",
    "    Returns:\n",
    "    A -- The activation values of current layer\n",
    "    cache -- A tuple containing both caches from linear part and activation part, (linear_cache, activation_cache)\n",
    "                where linear_cache is (A_prev, W, b), and activation_cache is Z\n",
    "    \"\"\"\n",
    "    \n",
    "    assert(W.shape[1]==A_prev.shape[0])\n",
    "    Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "    if activation == 'relu':        \n",
    "        A, activation_cache = relu(Z)\n",
    "    elif activation == 'sigmoid':\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    elif activation == 'softmax':\n",
    "        A, activation_cache = softmax(Z)\n",
    "    cache = (linear_cache, activation_cache)\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: [[9.15843267e-01]\n",
      " [4.81944495e-02]\n",
      " [3.58630571e-02]\n",
      " [9.92262631e-05]]\n",
      "cache: ((array([[1],\n",
      "       [2],\n",
      "       [3],\n",
      "       [4],\n",
      "       [5]]), array([[ 0.33024446,  0.69524729,  0.18531272, -0.41053868,  0.52340408],\n",
      "       [-0.03743498, -0.20204586,  1.31900975, -1.02470386,  0.17805087],\n",
      "       [-0.39225625,  0.94519371, -0.54069818,  0.42527993, -0.31315021],\n",
      "       [-0.3966761 ,  1.50437164, -0.33590766, -0.91941424, -0.76106843]]), array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]])), array([[ 3.25154286],\n",
      "       [ 0.30694147],\n",
      "       [ 0.01140533],\n",
      "       [-5.87865494]]))\n"
     ]
    }
   ],
   "source": [
    "# test linear_activation_forward\n",
    "layer_dims = [5, 4, 1]\n",
    "parameters = initialize_parameters(layer_dims)\n",
    "A_prev = np.array([1, 2, 3, 4, 5]).reshape((5, 1))\n",
    "W = parameters['W1']\n",
    "b = parameters['b1']\n",
    "A, cache = linear_activation_forward(A_prev, W, b, activation='softmax')\n",
    "print(\"A: \" + str(A))\n",
    "print(\"cache: \" + str(cache))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Use current parameters to calculate the output layer value AL.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data\n",
    "    parameters -- a dictionary containing W and b\n",
    "    \n",
    "    Returns:\n",
    "    AL -- output layer values \n",
    "    caches -- a list of cache, returned by linear_activation_forward() at each layer\n",
    "    \"\"\"\n",
    "    \n",
    "    caches = []\n",
    "    L = int(len(parameters) / 2)\n",
    "    A_prev = X\n",
    "    for l in range(1, L):\n",
    "        A_prev, cache = linear_activation_forward(A_prev, parameters['W'+str(l)], parameters['b'+str(l)], activation='relu')\n",
    "        caches.append(cache)\n",
    "    \n",
    "    # Last layer\n",
    "    AL, cache = linear_activation_forward(A_prev, parameters['W'+str(L)], parameters['b'+str(L)], activation='softmax')\n",
    "    caches.append(cache)\n",
    "    \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AL: [[0.33328926]\n",
      " [0.33340757]\n",
      " [0.33330317]]\n",
      "caches: 2\n"
     ]
    }
   ],
   "source": [
    "layer_dims = [5, 4, 3]\n",
    "parameters = initialize_parameters(layer_dims)\n",
    "X = np.array([1, 2, 3, 4, 5]).reshape((5, 1))\n",
    "AL, caches = L_model_forward(X, parameters)\n",
    "print('AL: ' + str(AL))\n",
    "print('caches: ' + str(len(caches)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compute cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Compute the cost function of softmax activation\n",
    "    \n",
    "    Argument:\n",
    "    AL -- hypothesis of the model\n",
    "    Y -- training labels\n",
    "    \n",
    "    Returns:\n",
    "    cost -- the value of cost function\n",
    "    \"\"\"\n",
    "    m = AL.shape[1]\n",
    "    cost = -1/m * np.sum(Y*np.log(AL))\n",
    "    cost = np.squeeze(cost)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost: 1.7919430854803733\n"
     ]
    }
   ],
   "source": [
    "layer_dims = [5, 4, 3]\n",
    "parameters = initialize_parameters(layer_dims)\n",
    "X = np.array([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]]).reshape((5, 2))\n",
    "Y = np.array([1, 0, 0, 0, 0, 1]).reshape((3, 2))\n",
    "AL, caches = L_model_forward(X, parameters)\n",
    "cost = compute_cost(AL, Y)\n",
    "print('cost: ' + str(cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Backward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Linear part of the backward propagation: \n",
    "    dW = 1/m * dZ * dA_prev.T (n[l], n[l-1]) (n[l], m) (n[l-1], m)\n",
    "    db = 1/m * np.sum(dZ, axis=1, keepdims=True)\n",
    "    \n",
    "    Args:\n",
    "    dZ -- derivative of cost function with respect to Z\n",
    "    cache -- cache from linear forward propagation, (A, W, b)\n",
    "    \n",
    "    Returns:\n",
    "    grads -- gradients used for gradient descent\n",
    "    \"\"\"\n",
    "    \n",
    "    m = dZ.shape[1]\n",
    "    dA_prev = cache[0]\n",
    "    \n",
    "    dW = 1/m * dZ @ dA_prev.T\n",
    "    db = 1/m * np.sum(dZ, axis=1, keepdims=True)\n",
    "    \n",
    "    grads = {'dW': dW,\n",
    "             'db': db}\n",
    "    \n",
    "    return grads    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grads: {'dW': array([[ 0.85210537,  1.06147179,  0.26176881, -0.95751962],\n",
      "       [ 1.70421073,  2.12294357,  0.52353762, -1.91503923],\n",
      "       [ 2.5563161 ,  3.18441536,  0.78530643, -2.87255885]]), 'db': array([[1.],\n",
      "       [2.],\n",
      "       [3.]])}\n"
     ]
    }
   ],
   "source": [
    "dZ = np.array([1, 2, 3]).reshape((3, 1))\n",
    "cache = (np.random.randn(4, 1), 0, 0)\n",
    "grads = linear_backward(dZ, cache)\n",
    "print('grads: ' + str(grads))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Linear + activation parts, whole backward propagation dA -> dA_prev\n",
    "    \n",
    "    Args:\n",
    "    dA -- derivative of cost function with respect to A\n",
    "    cache -- cache from linear activation forward propagation, (linear_cache, activation_cache)\n",
    "    \n",
    "    Returns:\n",
    "    grads -- gradients used for gradient descent\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == 'sigmoid':\n",
    "        dZ = sigmoid_backward(dA)\n",
    "    elif activation == 'relu':\n",
    "        dZ = relu_backward(dA)\n",
    "    elif activation == 'softmax':\n",
    "        dZ = softmax_backward(dA)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Update parameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
