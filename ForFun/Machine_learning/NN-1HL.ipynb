{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network with one hidden layer\n",
    "In this notebook I will implement a neural network with one hidden layer. This NN will then be used to classify the digital figures from [MNIST database](http://yann.lecun.com/exdb/mnist/). The output layer would be activated using \"softmax\" since the output is not just 0 or 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set dimensions: (60000, 28, 28)\n",
      "training label dimensions: (60000,)\n",
      "test set dimensions: (10000, 28, 28)\n",
      "test label dimensions: (10000,)\n"
     ]
    }
   ],
   "source": [
    "# data dims\n",
    "print('training set dimensions: ' + str(x_train.shape))\n",
    "print('training label dimensions: ' + str(y_train.shape))\n",
    "print('test set dimensions: ' + str(x_test.shape))\n",
    "print('test label dimensions: ' + str(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 helper functions\n",
    "- sigmoid\n",
    "- relu\n",
    "- softmax\n",
    "- sigmoid backward\n",
    "- relu backward\n",
    "- softmax backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x)), x\n",
    "def relu(x):\n",
    "    return np.maximum(0, x), x\n",
    "def softmax(x):\n",
    "    xx = x-x.max()\n",
    "    return np.exp(xx) / np.sum(np.exp(xx), axis=0, keepdims=True), x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape = (2, 5)\n",
      "y.shape = (2, 5)\n"
     ]
    }
   ],
   "source": [
    "# test helper functions\n",
    "x = np.random.randn(2, 5)\n",
    "y, cache = softmax(x)\n",
    "\n",
    "print('x.shape = ' + str(x.shape))\n",
    "print('y.shape = ' + str(y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    The backward version of sigmoid function, converting dL/dA to dL/dZ\n",
    "    \n",
    "    Args:\n",
    "    dA -- Derivative of cost function with respect to A, dL/dA\n",
    "    cache -- activation_cache, Z\n",
    "    \n",
    "    Returns:\n",
    "    dZ -- Derivative of cost function with respect to Z, dA * (dA/dZ)\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    A, _cache = sigmoid(Z)\n",
    "    \n",
    "    dAdZ = A * (1 - A)\n",
    "    dZ = dA * dAdZ\n",
    "    \n",
    "    return dZ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.19661193]\n",
      " [0.39322387]]\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "dA = np.array([1, 2]).reshape((2, 1))\n",
    "cache = np.array([1, 1]).reshape((2, 1))\n",
    "dZ = sigmoid_backward(dA, cache)\n",
    "print(dZ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    The backward version of relu function, converting dL/dA to dL/dZ\n",
    "    \n",
    "    Args:\n",
    "    dA -- Derivative of cost function with respect to A, dL/dA\n",
    "    cache -- activation_cache, Z\n",
    "    \n",
    "    Returns:\n",
    "    dZ -- Derivative of cost function with respect to Z, dA * (dA/dZ)\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    \n",
    "    dAdZ = Z >= 0\n",
    "    dZ = dA * dAdZ\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]\n",
      " [2]]\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "dA = np.array([1, 2]).reshape((2, 1))\n",
    "cache = np.array([1, 1]).reshape((2, 1))\n",
    "dZ = relu_backward(dA, cache)\n",
    "print(dZ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Derivative of softmax function:**\n",
    "\n",
    "Softmax function is defined as:\n",
    "\n",
    "$$ S(Z) = \\frac{\\exp Z}{\\sum_i \\exp z_i} $$ \n",
    "\n",
    "where Z is a column vector $\\left[\\begin{smallmatrix}z_1\\\\.\\\\.\\\\.\\\\z_n \\end{smallmatrix}\\right]$.\n",
    "\n",
    "The derivative of softmax function is:\n",
    "\n",
    "$$ \\frac{\\partial S_i}{\\partial Z_j} = \\begin{cases} S_i(1-S_j) & i \\neq j \\\\ -S_iS_j & i = j \\end{cases}$$\n",
    "\n",
    "This can be written in a more compact way:\n",
    "\n",
    "$$ \\frac{\\partial S_i}{\\partial Z_j} = S_i(\\delta_{ij} - S_j) $$\n",
    "\n",
    "If we define a broadcast version of $S$, whose shape is (n, 1), as $S^b$ with shape (n, n), where $S^b_{ij}=S_i$. Equivalently, $S^b_{ji}=S_j$. Since $S^b_{ji} = (S^b)^T_{ij}$, the derivative can be rewritten in matrix form:\n",
    "\n",
    "$$ \\frac{\\partial S}{\\partial Z} = S^b (I - (S^b)^T) $$\n",
    "\n",
    "$ \\frac{\\partial S}{\\partial Z} $ has shape (n, n). Up to now, we have assumed that S is a column vector with shape (n, 1). In a real application, we need to vectorize the computation for multiple training samples, then S will be of shape (n, m). The shape of $ \\frac{\\partial S}{\\partial Z} $ will be (n, n, m). \n",
    "\n",
    "To implement the vectorized calculation, we first broadcast S to S' with shape (n, n, m), where $S'(i, j, k) == S(i, k)$. Note that the computation for $S^b$ in the equation above is still valid for $S'$, except the transpose since we now have a 3-dimensional matrix. Here, we only take the transpose on the first two axes, leaving the 3rd dimension unchanged. \n",
    "\n",
    "$ \\frac{\\partial S}{\\partial Z} $ with shape (n, n, m) then needs to be used for computing $\\frac{\\partial L}{\\partial Z}$, where $L$ is the loss function of the model, by:\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial Z} = \\frac{\\partial L}{\\partial S} \\frac{\\partial S}{\\partial Z} $$\n",
    "\n",
    "again dimension analysis, $ \\frac{\\partial L}{\\partial Z} $ has shape (n, m), $ \\frac{\\partial L}{\\partial S} $ has shape (n, m), $ \\frac{\\partial S}{\\partial Z} $ has shape (n, n, m). We use Einstein sum (np.einsum) to do this computation as (ik, ijk -> jk) to obtain $ \\frac{\\partial L}{\\partial Z} $ of shape (n, m)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    The backward version of softmax function, converting dL/dA to dL/dZ \n",
    "    \n",
    "    Args:\n",
    "    dA -- Derivative of cost function with respect to A, dL/dA\n",
    "    cache -- activation_cache, Z\n",
    "    \n",
    "    Returns:\n",
    "    dZ -- Derivative of cost function with respect to Z, dA * (dA/dZ)\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    A, _cache = softmax(Z)\n",
    "    \n",
    "    n, m = Z.shape\n",
    "    \n",
    "    Ab = np.broadcast_to(np.expand_dims(A, axis=1), (n, n, m)) # broadcast matrix A for vectorized computation\n",
    "    AbT = np.transpose(Ab, axes=[1, 0, 2]) # transpose the first 2 axes    \n",
    "    I = np.identity(n)\n",
    "    Ib = np.broadcast_to(np.expand_dims(I, axis=2), (n, n, m))\n",
    "    \n",
    "    dAdZ = Ab * (Ib - AbT)\n",
    "    \n",
    "    # dA - (n, m)\n",
    "    # dAdZ - (n, n, m)\n",
    "    dZ = np.einsum('ik, ijk -> jk',   dA , dAdZ)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.0965267 ,  0.00696673,  0.02414509,  0.01036988],\n",
       "       [-0.03584462, -0.01716364, -0.02286152, -0.3106002 ]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dA = np.random.randn(2, 4)\n",
    "cache = np.random.randn(2, 4)\n",
    "softmax_backward(dA, cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test softmax backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = np.array([1, 2, 3, 4]).reshape((4, 1))\n",
    "A, cache = softmax(Z)\n",
    "Y = np.array([0, 0, 1, 0]).reshape((4, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "dA = - Y/A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "dZ = softmax_backward(dA, Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.0320586 ],\n",
       "       [ 0.08714432],\n",
       "       [-0.76311718],\n",
       "       [ 0.64391426]])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims):\n",
    "    \"\"\"\n",
    "    Initialize the parameters W's and b's.\n",
    "    \n",
    "    Argument:\n",
    "    layer_dims -- an array of layer dimensions [n0, n1, ..., nL]\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- a dictionary of initialized parameters\n",
    "                    W1: random\n",
    "                    b1: zeros\n",
    "                    ...\n",
    "                    WL: random\n",
    "                    bL: zeros\n",
    "    \"\"\"\n",
    "    parameters = {}\n",
    "    L = len(layer_dims) - 1\n",
    "    for l in range(0, L):\n",
    "        parameters['W'+str(l+1)] = np.random.randn(layer_dims[l+1], layer_dims[l]) * 0.01\n",
    "        parameters['b'+str(l+1)] = np.zeros((layer_dims[l+1], 1))\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'W1': array([[ 2.32250951e-03, -1.41330849e-03,  1.53493922e-02,\n",
      "        -3.41875301e-04,  2.81046192e-02],\n",
      "       [ 4.26460426e-03, -9.56055238e-03, -3.45585704e-04,\n",
      "        -1.51748050e-03,  1.40295434e-02],\n",
      "       [-9.74701231e-03,  8.93891403e-03, -1.40447513e-02,\n",
      "         2.03216436e-02, -2.64978169e-03],\n",
      "       [-7.13123573e-03, -1.00383172e-02, -1.27359826e-02,\n",
      "         9.21584587e-06, -2.69977082e-03]]), 'b1': array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]]), 'W2': array([[ 0.0005891 , -0.01011546,  0.02496523, -0.00260117]]), 'b2': array([[0.]])}\n"
     ]
    }
   ],
   "source": [
    "# test initialize_parameters()\n",
    "layer_dims = [5, 4, 1]\n",
    "parameters = initialize_parameters(layer_dims)\n",
    "print(parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear forward\n",
    "def linear_forward(A_prev, W, b):\n",
    "    \"\"\"\n",
    "    The linear part of forward propagation: Z = WA + b\n",
    "    \n",
    "    Argument:\n",
    "    A_prev -- the activitions of previous step (X for the first hidden layer)\n",
    "    W, b -- parameters \n",
    "    \n",
    "    Returns:\n",
    "    Z -- result of linear forward propagation\n",
    "    cache -- cache A_prev and the parameters W and b for convenience when calculating backpropagation\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = W @ A_prev + b\n",
    "    cache = (A_prev, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z: [[ 0.01531117]\n",
      " [-0.00145869]\n",
      " [ 0.06627346]\n",
      " [-0.06658928]]\n",
      "cache: (array([[1],\n",
      "       [2],\n",
      "       [3],\n",
      "       [4],\n",
      "       [5]]), array([[-6.99215352e-03,  2.60902500e-03,  4.63233218e-03,\n",
      "        -9.77979137e-03,  8.46148871e-03],\n",
      "       [ 9.31404769e-03, -9.43436747e-05,  6.26160176e-03,\n",
      "        -7.02047639e-03, -2.57390975e-04],\n",
      "       [ 2.00680757e-02,  5.51606103e-03,  4.04266822e-03,\n",
      "        -7.60821937e-03,  1.06956280e-02],\n",
      "       [ 2.14783427e-03, -5.58118132e-04, -1.51217944e-02,\n",
      "         1.47858133e-02, -1.62797506e-02]]), array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]]))\n"
     ]
    }
   ],
   "source": [
    "# test linear forward\n",
    "layer_dims = [5, 4, 1]\n",
    "parameters = initialize_parameters(layer_dims)\n",
    "A_prev = np.array([1, 2, 3, 4, 5]).reshape((5, 1))\n",
    "W = parameters['W1']\n",
    "b = parameters['b1']\n",
    "Z, cache = linear_forward(A_prev, W, b)\n",
    "print('Z: ' + str(Z))\n",
    "print('cache: ' + str(cache))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Forward propagation including both linear mapping and activation\n",
    "    \n",
    "    Argument:\n",
    "    A_prev -- the activations of previous step (X for the first hidden layer)\n",
    "    W, b -- parameters\n",
    "    activation -- activation function used in this layer\n",
    "    \n",
    "    Returns:\n",
    "    A -- The activation values of current layer\n",
    "    cache -- A tuple containing both caches from linear part and activation part, (linear_cache, activation_cache)\n",
    "                where linear_cache is (A_prev, W, b), and activation_cache is Z\n",
    "    \"\"\"\n",
    "    \n",
    "    assert(W.shape[1]==A_prev.shape[0])\n",
    "    Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "    if activation == 'relu':        \n",
    "        A, activation_cache = relu(Z)\n",
    "    elif activation == 'sigmoid':\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    elif activation == 'softmax':\n",
    "        A, activation_cache = softmax(Z)\n",
    "    cache = (linear_cache, activation_cache)\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: [[0.14119504]\n",
      " [0.17185106]\n",
      " [0.        ]\n",
      " [0.        ]]\n",
      "cache: ((array([[1],\n",
      "       [2],\n",
      "       [3],\n",
      "       [4],\n",
      "       [5]]), array([[ 1.06598244e-02,  5.98888751e-03,  1.32266946e-02,\n",
      "         9.72867180e-03,  7.99253427e-03],\n",
      "       [-1.05900787e-02,  1.01312824e-02,  2.06294928e-02,\n",
      "         8.17826228e-04,  1.94037588e-02],\n",
      "       [ 3.70293003e-03, -1.59178088e-03, -1.05235848e-02,\n",
      "         3.15753806e-03, -7.72343064e-03],\n",
      "       [-5.98971566e-03,  1.21650836e-02, -1.06929208e-02,\n",
      "        -1.22663508e-02,  7.82467729e-05]]), array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]])), array([[ 0.14119504],\n",
      "       [ 0.17185106],\n",
      "       [-0.05703839],\n",
      "       [-0.06241248]]))\n"
     ]
    }
   ],
   "source": [
    "# test linear_activation_forward\n",
    "layer_dims = [5, 4, 1]\n",
    "parameters = initialize_parameters(layer_dims)\n",
    "A_prev = np.array([1, 2, 3, 4, 5]).reshape((5, 1))\n",
    "W = parameters['W1']\n",
    "b = parameters['b1']\n",
    "A, cache = linear_activation_forward(A_prev, W, b, activation='relu')\n",
    "print(\"A: \" + str(A))\n",
    "print(\"cache: \" + str(cache))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00501261,  0.00380598,  0.00512525, -0.01401253,  0.01814257],\n",
       "       [-0.02207838,  0.01404174,  0.0016276 , -0.00527018, -0.009393  ],\n",
       "       [ 0.01976955, -0.0085519 , -0.0061324 ,  0.00619611, -0.01525609],\n",
       "       [ 0.00069867,  0.00346226, -0.01570748,  0.00297798,  0.00301298]])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Use current parameters to calculate the output layer value AL.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data\n",
    "    parameters -- a dictionary containing W and b\n",
    "    \n",
    "    Returns:\n",
    "    AL -- output layer values \n",
    "    caches -- a list of cache, returned by linear_activation_forward() at each layer\n",
    "    \"\"\"\n",
    "    \n",
    "    caches = []\n",
    "    L = int(len(parameters) / 2)\n",
    "    A_prev = X\n",
    "    for l in range(1, L):\n",
    "        A_prev, cache = linear_activation_forward(A_prev, parameters['W'+str(l)], parameters['b'+str(l)], activation='sigmoid')\n",
    "        caches.append(cache)\n",
    "    \n",
    "    \n",
    "    # Last layer\n",
    "    AL, cache = linear_activation_forward(A_prev, parameters['W'+str(L)], parameters['b'+str(L)], activation='softmax')\n",
    "    caches.append(cache)\n",
    "    \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AL: [[0.33294258]\n",
      " [0.33354824]\n",
      " [0.33350918]]\n",
      "caches: 2\n"
     ]
    }
   ],
   "source": [
    "layer_dims = [5, 4, 3]\n",
    "parameters = initialize_parameters(layer_dims)\n",
    "X = np.array([1, 2, 3, 4, 5]).reshape((5, 1))\n",
    "AL, caches = L_model_forward(X, parameters)\n",
    "print('AL: ' + str(AL))\n",
    "print('caches: ' + str(len(caches)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compute cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Compute the cost function of softmax activation\n",
    "    \n",
    "    Argument:\n",
    "    AL -- hypothesis of the model\n",
    "    Y -- training labels\n",
    "    \n",
    "    Returns:\n",
    "    cost -- the value of cost function\n",
    "    \"\"\"\n",
    "    m = AL.shape[1]\n",
    "    cost = -1/m * np.sum(Y*np.log(AL))\n",
    "    cost = np.squeeze(cost)\n",
    "    return cost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost: 1.7928384245030298\n"
     ]
    }
   ],
   "source": [
    "layer_dims = [5, 4, 3]\n",
    "parameters = initialize_parameters(layer_dims)\n",
    "X = np.array([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]]).reshape((5, 2))\n",
    "Y = np.array([1, 0, 0, 0, 0, 1]).reshape((3, 2))\n",
    "AL, caches = L_model_forward(X, parameters)\n",
    "cost = compute_cost(AL, Y)\n",
    "print('cost: ' + str(cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understand softmax cost function\n",
    "\n",
    "The cost function of softmax function is:\n",
    "\n",
    "$$ J(A, Y) = - \\frac{1}{m} \\sum_j^m\\sum_i^n y_i^{(j)}\\log a_i^{(j)}$$\n",
    "\n",
    "where $A = \\left[\\begin{smallmatrix} a_1^{(1)} & .. & a_1^{(m)} \\\\ .&.&. \\\\ a_n^{(1)} & .. & a_n^{(m)} \\end{smallmatrix} \\right]$, $Y = \\left[\\begin{smallmatrix} y_1^{(1)} & .. & y_1^{(m)} \\\\ .&.&. \\\\ y_n^{(1)} & .. & y_n^{(m)} \\end{smallmatrix} \\right]$\n",
    "\n",
    "For training sample (j), the loss function is \n",
    "\n",
    "$$ J( a^{(j)}, y^{(j)}) = -\\sum_i^n y_i^{(j)}\\log a_i^{(j)} $$\n",
    "\n",
    "Take an exponential of the RHS\n",
    "\n",
    "$$ \\exp\\left(\\sum_i^n y_i^{(j)}\\log a_i^{(j)}\\right) = \\prod_i^n (a_i^{(j)})^{-y_i^{(j)}}$$\n",
    "\n",
    "Since there is only one $y_i^{(j)}$ for a certain (j) which is 1, whereas all others are 0, we can minimize the loss function by setting the corresponding $a_i^{(j)}$ large. This is equivalent to the goal of making $a_i^{(j)}$ and $y_i^{(j)}$ equal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Backward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear part back propagation:**\n",
    "\n",
    "Convert the derivative of cost function with respect to $Z$, the neuron value before activation, $\\frac{\\partial L}{\\partial Z}$, to the derivative with respect to parameters $W$ and $b$, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Linear part of the backward propagation: \n",
    "    dW = 1/m * dZ * dA_prev.T (n[l], n[l-1]) (n[l], m) (n[l-1], m)\n",
    "    db = 1/m * np.sum(dZ, axis=1, keepdims=True)\n",
    "    \n",
    "    Args:\n",
    "    dZ -- derivative of cost function with respect to Z\n",
    "    cache -- cache from linear forward propagation, (A, W, b)\n",
    "    \n",
    "    Returns:\n",
    "    dW, db -- gradients used for gradient descent\n",
    "    \"\"\"\n",
    "    \n",
    "    m = dZ.shape[1]\n",
    "    A_prev = cache[0]\n",
    "    \n",
    "    dW = 1/m * dZ @ A_prev.T\n",
    "    db = 1/m * np.sum(dZ, axis=1, keepdims=True)\n",
    "    \n",
    "    \n",
    "    return dW, db "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW: [[ 1.21388892  1.85144802 -1.15620048  0.39800219]\n",
      " [ 2.42777784  3.70289604 -2.31240096  0.79600439]\n",
      " [ 3.64166676  5.55434406 -3.46860144  1.19400658]]\n"
     ]
    }
   ],
   "source": [
    "dZ = np.array([1, 2, 3]).reshape((3, 1))\n",
    "cache = (np.random.randn(4, 1), 0, 0)\n",
    "dW, db = linear_backward(dZ, cache)\n",
    "print('dW: ' + str(dW))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Linear + activation parts, whole backward propagation dA -> dA_prev\n",
    "    \n",
    "    Args:\n",
    "    dA -- derivative of cost function with respect to A\n",
    "    cache -- cache from linear activation forward propagation, (linear_cache, activation_cache)\n",
    "    \n",
    "    Returns:\n",
    "    dW, db -- gradients used for gradient descent\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    W = linear_cache[1]\n",
    "    \n",
    "    if activation == 'sigmoid':\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "    elif activation == 'relu':\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "    elif activation == 'softmax':\n",
    "        dZ = softmax_backward(dA, activation_cache)\n",
    "    \n",
    "    dA_prev = W.T @ dZ\n",
    "    \n",
    "    dW, db = linear_backward(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW: [[-0.4207154   0.65934649  0.05163552 -0.45783475]\n",
      " [-0.8414308   1.31869299  0.10327105 -0.9156695 ]\n",
      " [-1.26214619  1.97803948  0.15490657 -1.37350424]]\n"
     ]
    }
   ],
   "source": [
    "dA = np.array([1, 2, 3]).reshape((3, 1))\n",
    "cache = ((np.random.randn(4, 1), np.random.randn(3, 4), 0), np.random.randn(3, 1))\n",
    "dA_prev, dW, db = linear_activation_backward(dZ, cache, 'relu')\n",
    "print('dW: ' + str(dW))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    AL -- activation values of the last layer\n",
    "    Y -- data labels\n",
    "    caches -- a list of caches from each layer, layer one cache corresponds to caches[0]\n",
    "    \n",
    "    Returns:\n",
    "    grads -- a dict of gradients (of loss function wrt parameters) in each layer, will be used to update the parameters.\n",
    "    \"\"\"\n",
    "    \n",
    "    assert(Y.shape == AL.shape)\n",
    "    assert((AL != 0).all())\n",
    "    grads = {}\n",
    "    m = Y.shape[1]\n",
    "    dAL = - (Y / AL)\n",
    "    m = Y.shape[1]\n",
    "    L = len(caches)\n",
    "    \n",
    "    cache = caches[L-1]\n",
    "#     print('AL='+ str(AL))\n",
    "    grads['dA'+str(L-1)], grads['dW'+str(L)], grads['db'+str(L)] = linear_activation_backward(dAL, cache, 'softmax')    \n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        grads['dA'+str(l)], grads['dW'+str(l+1)], grads['db'+str(l+1)] = linear_activation_backward(grads['dA'+str(l+1)], caches[l], 'sigmoid')\n",
    "        \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True]])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A != 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Update parameters\n",
    "\n",
    "Update parameters W and b using the grads value returned by ```linear_activation_backward()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    parameters -- current parameter W and b\n",
    "    grads -- gradients of loss function wrt W and b \n",
    "    learning_rate -- rate of gradient descent\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- updated W and b\n",
    "    \"\"\"\n",
    "\n",
    "    L = len(parameters) // 2\n",
    "    \n",
    "    for l in range(L):    \n",
    "        parameters['W' + str(l+1)] -= learning_rate * grads['dW' + str(l+1)]\n",
    "        parameters['b' + str(l+1)] -= learning_rate * grads['db' + str(l+1)]\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test update_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**transform Y**\n",
    "\n",
    "The given Y has shape (1, 60000), where each column corresponds to the label digit of its corresponding image. In our learning algorithm, we use softmax to activate the last layer in order to get 10 output and compute a reasonable cost. Previously, I have been using Y with shape (1, 60000) directly for ```L_model_backward()``` but no error occured. It was because the (1, 60000) matrix was automatically broadcast to (10, 60000) which is the shape of AL.\n",
    "\n",
    "Therefore, I add one more assertion to avoid this bug to ```L_model_backward()```: \n",
    "\n",
    "```python\n",
    "assert(Y.shape == AL.shape)\n",
    "```\n",
    "\n",
    "In order to make the learning algorithm work, I need to transform Y to a (10, 60000) matrix, where the numbers in the original array indicate the row locations of 1's in the new matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Iterate the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, layer_dims, learning_rate, num_iterations=1000, print_cost=False):\n",
    "    L = len(layer_dims) - 1\n",
    "    n0, m = X.shape\n",
    "    parameters = initialize_parameters(layer_dims)\n",
    "    for i in range(num_iterations):\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "#         print('AL.shape: ' + str(AL.shape))\n",
    "        \n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    "#         print('grads: ' + str(grads))\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "#         print('parameters updated')\n",
    "        cost = compute_cost(AL, Y)\n",
    "        if print_cost and i % 10 == 0:\n",
    "            print('Iteration {0} cost: {1}'.format(i, cost))\n",
    "#             print('AL: ' + str(AL))\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 cost: 2.3000857205811975\n",
      "Iteration 10 cost: 2.2747131245164827\n",
      "Iteration 20 cost: 2.245573708892367\n",
      "Iteration 30 cost: 2.211095257017025\n",
      "Iteration 40 cost: 2.171216846565133\n",
      "Iteration 50 cost: 2.1278788947722354\n",
      "Iteration 60 cost: 2.0826484328856956\n",
      "Iteration 70 cost: 2.0374711319266114\n",
      "Iteration 80 cost: 1.9923208861108583\n",
      "Iteration 90 cost: 1.947391381808716\n",
      "Iteration 100 cost: 1.9029706973368135\n",
      "Iteration 110 cost: 1.859363859061456\n",
      "Iteration 120 cost: 1.8166929927555078\n",
      "Iteration 130 cost: 1.7752090656844708\n",
      "Iteration 140 cost: 1.7349960236978912\n",
      "Iteration 150 cost: 1.6960243466374294\n",
      "Iteration 160 cost: 1.6583329454500144\n",
      "Iteration 170 cost: 1.622583030240086\n",
      "Iteration 180 cost: 1.5889682590831908\n",
      "Iteration 190 cost: 1.554682390654763\n",
      "Iteration 200 cost: 1.5221368081384536\n",
      "Iteration 210 cost: 1.4914358697084447\n",
      "Iteration 220 cost: 1.4629776423611665\n",
      "Iteration 230 cost: 1.4318216472807381\n",
      "Iteration 240 cost: 1.4049837422893443\n",
      "Iteration 250 cost: 1.3820081168783036\n",
      "Iteration 260 cost: 1.3492151242134192\n",
      "Iteration 270 cost: 1.3246637644825883\n",
      "Iteration 280 cost: 1.3106213115691683\n",
      "Iteration 290 cost: 1.2774511075390622\n",
      "Iteration 300 cost: 1.2514756492498875\n",
      "Iteration 310 cost: 1.2479145353781316\n",
      "Iteration 320 cost: 1.2140480897044328\n",
      "Iteration 330 cost: 1.188567499489979\n",
      "Iteration 340 cost: 1.1924282899649294\n",
      "Iteration 350 cost: 1.1534007330296263\n",
      "Iteration 360 cost: 1.1282228216057537\n",
      "Iteration 370 cost: 1.1362428545281031\n",
      "Iteration 380 cost: 1.1005530284698333\n",
      "Iteration 390 cost: 1.0763279073626801\n",
      "Iteration 400 cost: 1.0815079890250148\n",
      "Iteration 410 cost: 1.0568206587455493\n",
      "Iteration 420 cost: 1.0276453026220407\n",
      "Iteration 430 cost: 1.0335493535031735\n",
      "Iteration 440 cost: 1.0146880392528237\n",
      "Iteration 450 cost: 0.9847577284237152\n",
      "Iteration 460 cost: 0.9891927617099765\n",
      "Iteration 470 cost: 0.9817665312144662\n",
      "Iteration 480 cost: 0.9430802080773232\n",
      "Iteration 490 cost: 0.9474726743861882\n",
      "Iteration 500 cost: 0.9248208100817364\n",
      "Iteration 510 cost: 0.9203673272303993\n",
      "Iteration 520 cost: 0.924291383260963\n",
      "Iteration 530 cost: 0.8858151429498458\n",
      "Iteration 540 cost: 0.8869785903858319\n",
      "Iteration 550 cost: 0.8690100574603228\n",
      "Iteration 560 cost: 0.879888743868058\n",
      "Iteration 570 cost: 0.8539386548721429\n",
      "Iteration 580 cost: 0.83512377253396\n",
      "Iteration 590 cost: 0.853104230484176\n",
      "Iteration 600 cost: 0.8182258613456578\n",
      "Iteration 610 cost: 0.8049005766553975\n",
      "Iteration 620 cost: 0.837020050347335\n",
      "Iteration 630 cost: 0.7950765989641576\n",
      "Iteration 640 cost: 0.7825477053586534\n",
      "Iteration 650 cost: 0.7877917587746357\n",
      "Iteration 660 cost: 0.781259594801525\n",
      "Iteration 670 cost: 0.7545083810483905\n",
      "Iteration 680 cost: 0.7613380636907933\n",
      "Iteration 690 cost: 0.7647279506117448\n",
      "Iteration 700 cost: 0.7312556783296362\n",
      "Iteration 710 cost: 0.7413329060404278\n",
      "Iteration 720 cost: 0.7300189571176525\n",
      "Iteration 730 cost: 0.7127571728417957\n",
      "Iteration 740 cost: 0.7264199476166181\n",
      "Iteration 750 cost: 0.714781048085699\n",
      "Iteration 760 cost: 0.6921170600653629\n",
      "Iteration 770 cost: 0.7123693755718534\n",
      "Iteration 780 cost: 0.6801473225933344\n",
      "Iteration 790 cost: 0.6828078390722433\n",
      "Iteration 800 cost: 0.6834059978407564\n",
      "Iteration 810 cost: 0.6676098003121193\n",
      "Iteration 820 cost: 0.6583756151988429\n",
      "Iteration 830 cost: 0.6814894420485097\n",
      "Iteration 840 cost: 0.6454750486320956\n",
      "Iteration 850 cost: 0.6641302858492885\n",
      "Iteration 860 cost: 0.6484516750420652\n",
      "Iteration 870 cost: 0.6344554875090315\n",
      "Iteration 880 cost: 0.6392794986997714\n",
      "Iteration 890 cost: 0.6303610642797847\n",
      "Iteration 900 cost: 0.6462136293172888\n",
      "Iteration 910 cost: 0.6126946877002543\n",
      "Iteration 920 cost: 0.6213828910713214\n",
      "Iteration 930 cost: 0.6306448346370398\n",
      "Iteration 940 cost: 0.5983044158239701\n",
      "Iteration 950 cost: 0.6095614603482106\n",
      "Iteration 960 cost: 0.601938389870082\n",
      "Iteration 970 cost: 0.5927466229862376\n",
      "Iteration 980 cost: 0.6040821748358265\n",
      "Iteration 990 cost: 0.585389116192357\n"
     ]
    }
   ],
   "source": [
    "m = x_train.shape[0]\n",
    "r, c = x_train.shape[1], x_train.shape[1]\n",
    "X = x_train.reshape((m, r*c))\n",
    "X = X.T\n",
    "Y1 = y_train.reshape((1, m))\n",
    "n, m = 10, Y1.shape[1]\n",
    "Y = np.zeros((n, m))\n",
    "Y[Y1, np.arange(m)] = 1\n",
    "layer_dims = [r*c, 20, 10]\n",
    "parameters = L_layer_model(X, Y, layer_dims, learning_rate=0.02, num_iterations=1000, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[-0.01078314, -0.01047815,  0.00673244, ...,  0.01084923,\n",
       "          0.00692201, -0.0057775 ],\n",
       "        [-0.00836306, -0.00770925,  0.00698634, ...,  0.00262784,\n",
       "         -0.01173791,  0.00210835],\n",
       "        [-0.00615542,  0.00854349, -0.01742107, ...,  0.00927265,\n",
       "         -0.00068873,  0.00160297],\n",
       "        ...,\n",
       "        [ 0.01070821,  0.00487171,  0.00148207, ...,  0.00563985,\n",
       "         -0.01046456, -0.02628802],\n",
       "        [ 0.00251906, -0.01134621, -0.00463086, ..., -0.00281211,\n",
       "         -0.00998264, -0.01022773],\n",
       "        [ 0.0040811 , -0.00761623,  0.01848787, ...,  0.00907349,\n",
       "          0.00242964, -0.00521692]]),\n",
       " 'b1': array([[ 1.00837745e-04],\n",
       "        [-5.20284952e-04],\n",
       "        [-2.90167012e-04],\n",
       "        [ 3.91866838e-04],\n",
       "        [-3.51017601e-04],\n",
       "        [-1.69597544e-04],\n",
       "        [ 4.90965003e-04],\n",
       "        [-8.45840388e-05],\n",
       "        [-9.31288621e-04],\n",
       "        [ 1.34999738e-04],\n",
       "        [-1.82012903e-04],\n",
       "        [-2.49311336e-04],\n",
       "        [ 3.41523291e-04],\n",
       "        [ 5.44567527e-04],\n",
       "        [ 1.98705084e-04],\n",
       "        [ 1.15965784e-04],\n",
       "        [-2.21759770e-04],\n",
       "        [ 3.48928170e-04],\n",
       "        [-5.82401680e-05],\n",
       "        [ 5.20077896e-04]]),\n",
       " 'W2': array([[-0.41680596,  0.40544547,  0.28677153,  0.22524509, -0.49276429,\n",
       "         -0.3469728 ,  0.4677753 ,  0.20284174, -0.44730772, -0.42406253,\n",
       "          0.25422667,  0.28839478, -0.17308661,  0.51699277, -0.49537798,\n",
       "         -0.24192062, -0.49487299, -0.29144635,  0.5483918 ,  0.36590048],\n",
       "        [ 0.39511183,  0.35594023, -0.53891239,  0.13053473,  0.27587996,\n",
       "          0.43587344, -0.36037182, -0.67544847, -0.41957968, -0.37867275,\n",
       "          0.22393562,  0.14652463,  0.67870089, -0.28799633,  0.22192528,\n",
       "         -0.23795112,  0.37815686,  0.52056512,  0.34868941, -0.53992954],\n",
       "        [ 0.36108905,  0.31280135,  0.30011436,  0.04181041,  0.30882533,\n",
       "          0.40469562, -0.24015068, -0.71796436,  0.32035935,  0.45880684,\n",
       "          0.17923845,  0.11700385, -0.16662734, -0.29191044, -0.58882665,\n",
       "         -0.13235173, -0.44536367, -0.26671597,  0.53694095,  0.26983672],\n",
       "        [-0.36424057, -0.39830937, -0.58370016,  0.36311787,  0.39451077,\n",
       "          0.44667986, -0.35322901,  0.25759533,  0.45121182, -0.26474853,\n",
       "          0.26427326,  0.26325972, -0.18465912, -0.32541657,  0.31732489,\n",
       "         -0.12825881, -0.55290575, -0.2689405 , -0.09620863,  0.419685  ],\n",
       "        [ 0.67073504, -0.32822146,  0.17432806, -0.57816173, -0.26224908,\n",
       "         -0.28075829,  0.30239693,  0.13980417,  0.14913289,  0.30039169,\n",
       "         -0.39538459, -0.42112337, -0.3018318 ,  0.30504107, -0.01652936,\n",
       "          0.45384446,  0.29266068,  0.34265118, -0.15795563, -0.23820382],\n",
       "        [-0.17753256, -0.29484983, -0.24596241, -0.19126901,  0.33236885,\n",
       "         -0.32317686, -0.27231492,  0.35201047, -0.47180897, -0.2933079 ,\n",
       "          0.31026833,  0.39621502, -0.13780586,  0.10743377,  0.29734213,\n",
       "         -0.1794155 ,  0.26921213, -0.28959322, -0.26738653,  0.51582988],\n",
       "        [ 0.45834534,  0.43417678,  0.24552935,  0.25469305, -0.29344763,\n",
       "          0.49243839,  0.37850227,  0.214241  , -0.52039612,  0.31034043,\n",
       "         -0.53175361, -0.43745601, -0.14880646, -0.3816435 , -0.54043558,\n",
       "         -0.27849341,  0.3979279 , -0.32730229, -0.32001812,  0.37889127],\n",
       "        [-0.40670761, -0.32460051,  0.20034568,  0.48254198, -0.2837039 ,\n",
       "         -0.19347537,  0.16069291, -0.1837872 ,  0.25464133,  0.32225216,\n",
       "         -0.31807472, -0.39491581,  0.82539077,  0.32159787,  0.26237569,\n",
       "          0.48030578, -0.61038262,  0.35983246, -0.14368225, -0.34886047],\n",
       "        [-0.17242309,  0.17708733,  0.41913232, -0.27683302,  0.35705071,\n",
       "         -0.33855102, -0.35636458,  0.2154719 ,  0.36025156, -0.33494613,\n",
       "          0.30138237,  0.36844137, -0.19113949, -0.34461649,  0.30155374,\n",
       "         -0.28286067,  0.3870694 , -0.1887387 , -0.2990839 , -0.49368378],\n",
       "        [-0.39612725, -0.31234341, -0.25454855, -0.4302502 , -0.28785132,\n",
       "         -0.27286972,  0.23105398,  0.2261079 ,  0.330432  ,  0.34330901,\n",
       "         -0.31878048, -0.3736624 , -0.25183575,  0.36263149,  0.22786641,\n",
       "          0.49738218,  0.37851514,  0.41139382, -0.15608956, -0.2988108 ]]),\n",
       " 'b2': array([[-0.04886563],\n",
       "        [-0.08044524],\n",
       "        [-0.04824086],\n",
       "        [ 0.03864398],\n",
       "        [-0.05179756],\n",
       "        [ 0.15830293],\n",
       "        [-0.01895909],\n",
       "        [-0.00891984],\n",
       "        [ 0.02323419],\n",
       "        [ 0.03704713]])}"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Numerical instability**\n",
    "When iterating softmax classification, we encountered numerical instability where np.exp overflows. This happens typically because the Z values sometimes get too large. To avoid this, we look at the formula for computing softmax again:\n",
    "\n",
    "$$ S_j = \\frac{\\exp z_j}{\\sum_i \\exp z_i}$$\n",
    "\n",
    "if we multiply the numerator and denominator both with a constant real number C, the formula becomes\n",
    "\n",
    "$$ S_j = \\frac{C\\exp z_j}{\\sum_i C\\exp z_i}  = \\frac{\\exp z_j+\\log C}{\\sum_i \\exp z_i + \\log C}$$\n",
    "\n",
    "The overflow of $\\exp z_j$ is cause by large $z_j$. We see in the above equation that by adding a same constant to all $z$'s does not influence the output value of softmax function. Therefore, we modify the softmax function as follows: \n",
    "\n",
    "$$ S_j = \\frac{\\exp z_j - \\max(Z)}{\\sum_i \\exp z_i - \\max(Z)}$$\n",
    "\n",
    "<del>This turns out not very helpful.</del> The issue is the definition of the ```softmax()```. After changing it, the algorithm starts to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.transpose(x_test, axes=[1, 2, 0]).reshape((28*28, 10000))\n",
    "A, cache = L_model_forward(X_test, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "[0.01822289 0.02357232 0.0120256  0.01872075 0.43431835 0.0591388\n",
      " 0.01819358 0.06369247 0.13205727 0.22005797]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x17716ee0d08>"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 6\n",
    "print(np.argmax(A[:, index]))\n",
    "print(A[:, index])\n",
    "plt.imshow(X[:, index].reshape((r, c)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_softmax(X_test, Y_test, parameters):\n",
    "    \"\"\"\n",
    "    Test the accuracy of model's prediction.\n",
    "    \n",
    "    Args:\n",
    "    X_test -- input features of the dataset to be tested\n",
    "    Y_test -- labels of the dataset ot be tested\n",
    "    parameters -- parameters of the trained model\n",
    "    \n",
    "    Returns:\n",
    "    predictions -- a prediction array of the output, have the same shape as Y_test\n",
    "    \"\"\"\n",
    "    assert(X_test.shape[1] == Y_test.shape[1])\n",
    "    \n",
    "    m = X_test.shape[1]\n",
    "    A, cache = L_model_forward(X_test, parameters)\n",
    "    predictions = np.argmax(A, axis=0)\n",
    "    check = predictions == Y_test\n",
    "    num_correct = np.nonzero(check)[0].shape[0]\n",
    "    accuracy = num_correct / m\n",
    "    print('Accuracy: ' + str(accuracy))\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.901\n"
     ]
    }
   ],
   "source": [
    "X_test = np.transpose(x_test, axes=[1, 2, 0]).reshape((28*28, 10000))\n",
    "Y_test = y_test.reshape((1, len(y_test)))\n",
    "predictions = predict_softmax(X_test, Y_test, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1, ..., 4, 5, 6], dtype=int64)"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1771732f4c8>"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 3000\n",
    "plt.imshow(x_test[index, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.nonzero(np.array([True, True, True, False]))[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 60000)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
