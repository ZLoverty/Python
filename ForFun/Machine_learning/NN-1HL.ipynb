{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network with one hidden layer\n",
    "In this notebook I will implement a neural network with one hidden layer. This NN will then be used to classify the digital figures from [MNIST database](http://yann.lecun.com/exdb/mnist/). The output layer would be activated using \"softmax\" since the output is not just 0 or 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set dimensions: (60000, 28, 28)\n",
      "training label dimensions: (60000,)\n",
      "test set dimensions: (10000, 28, 28)\n",
      "test label dimensions: (10000,)\n"
     ]
    }
   ],
   "source": [
    "# data dims\n",
    "print('training set dimensions: ' + str(x_train.shape))\n",
    "print('training label dimensions: ' + str(y_train.shape))\n",
    "print('test set dimensions: ' + str(x_test.shape))\n",
    "print('test label dimensions: ' + str(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 helper functions\n",
    "- sigmoid\n",
    "- relu\n",
    "- softmax\n",
    "- sigmoid backward\n",
    "- relu backward\n",
    "- softmax backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x)), x\n",
    "def relu(x):\n",
    "    return np.maximum(0, x), x\n",
    "def softmax(x):\n",
    "    return np.exp(x-x.max()) / np.sum(np.exp(x-x.max())), x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape = (2, 5)\n",
      "y.shape = (2, 5)\n"
     ]
    }
   ],
   "source": [
    "# test helper functions\n",
    "x = np.arange(10).reshape((2, 5))\n",
    "y, cache = softmax(x)\n",
    "\n",
    "print('x.shape = ' + str(x.shape))\n",
    "print('y.shape = ' + str(y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    The backward version of sigmoid function, converting dL/dA to dL/dZ\n",
    "    \n",
    "    Args:\n",
    "    dA -- Derivative of cost function with respect to A, dL/dA\n",
    "    cache -- activation_cache, Z\n",
    "    \n",
    "    Returns:\n",
    "    dZ -- Derivative of cost function with respect to Z, dA * (dA/dZ)\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    A, _cache = sigmoid(Z)\n",
    "    \n",
    "    dAdZ = A * (1 - A)\n",
    "    dZ = dA * dAdZ\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.19661193]\n",
      " [0.39322387]]\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "dA = np.array([1, 2]).reshape((2, 1))\n",
    "cache = np.array([1, 1]).reshape((2, 1))\n",
    "dZ = sigmoid_backward(dA, cache)\n",
    "print(dZ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    The backward version of relu function, converting dL/dA to dL/dZ\n",
    "    \n",
    "    Args:\n",
    "    dA -- Derivative of cost function with respect to A, dL/dA\n",
    "    cache -- activation_cache, Z\n",
    "    \n",
    "    Returns:\n",
    "    dZ -- Derivative of cost function with respect to Z, dA * (dA/dZ)\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    \n",
    "    dAdZ = Z >= 0\n",
    "    dZ = dA * dAdZ\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]\n",
      " [2]]\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "dA = np.array([1, 2]).reshape((2, 1))\n",
    "cache = np.array([1, 1]).reshape((2, 1))\n",
    "dZ = relu_backward(dA, cache)\n",
    "print(dZ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Derivative of softmax function:**\n",
    "\n",
    "Softmax function is defined as:\n",
    "\n",
    "$$ S(Z) = \\frac{\\exp Z}{\\sum_i \\exp z_i} $$ \n",
    "\n",
    "where Z is a column vector $\\left[\\begin{smallmatrix}z_1\\\\.\\\\.\\\\.\\\\z_n \\end{smallmatrix}\\right]$.\n",
    "\n",
    "The derivative of softmax function is:\n",
    "\n",
    "$$ \\frac{\\partial S_i}{\\partial Z_j} = \\begin{cases} S_i(1-S_j) & i \\neq j \\\\ -S_iS_j & i = j \\end{cases}$$\n",
    "\n",
    "This can be written in a more compact way:\n",
    "\n",
    "$$ \\frac{\\partial S_i}{\\partial Z_j} = S_i(\\delta_{ij} - S_j) $$\n",
    "\n",
    "If we define a broadcast version of $S$, whose shape is (n, 1), as $S^b$ with shape (n, n), where $S^b_{ij}=S_i$. Equivalently, $S^b_{ji}=S_j$. Since $S^b_{ji} = (S^b)^T_{ij}$, the derivative can be rewritten in matrix form:\n",
    "\n",
    "$$ \\frac{\\partial S}{\\partial Z} = S^b (I - (S^b)^T) $$\n",
    "\n",
    "$ \\frac{\\partial S}{\\partial Z} $ has shape (n, n). Up to now, we have assumed that S is a column vector with shape (n, 1). In a real application, we need to vectorize the computation for multiple training samples, then S will be of shape (n, m). The shape of $ \\frac{\\partial S}{\\partial Z} $ will be (n, n, m). \n",
    "\n",
    "To implement the vectorized calculation, we first broadcast S to S' with shape (n, n, m), where $S'(i, j, k) == S(i, k)$. Note that the computation for $S^b$ in the equation above is still valid for $S'$, except the transpose since we now have a 3-dimensional matrix. Here, we only take the transpose on the first two axes, leaving the 3rd dimension unchanged. \n",
    "\n",
    "$ \\frac{\\partial S}{\\partial Z} $ with shape (n, n, m) then needs to be used for computing $\\frac{\\partial L}{\\partial Z}$, where $L$ is the loss function of the model, by:\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial Z} = \\frac{\\partial L}{\\partial S} \\frac{\\partial S}{\\partial Z} $$\n",
    "\n",
    "again dimension analysis, $ \\frac{\\partial L}{\\partial Z} $ has shape (n, m), $ \\frac{\\partial L}{\\partial S} $ has shape (n, m), $ \\frac{\\partial S}{\\partial Z} $ has shape (n, n, m). We use Einstein sum (np.einsum) to do this computation as (ik, ijk -> jk) to obtain $ \\frac{\\partial L}{\\partial Z} $ of shape (n, m)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    The backward version of softmax function, converting dL/dA to dL/dZ \n",
    "    \n",
    "    Args:\n",
    "    dA -- Derivative of cost function with respect to A, dL/dA\n",
    "    cache -- activation_cache, Z\n",
    "    \n",
    "    Returns:\n",
    "    dZ -- Derivative of cost function with respect to Z, dA * (dA/dZ)\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    A, _cache = softmax(Z)\n",
    "    \n",
    "    n, m = Z.shape\n",
    "    \n",
    "    Ab = np.broadcast_to(np.expand_dims(A, axis=1), (n, n, m)) # broadcast matrix A for vectorized computation\n",
    "    AbT = np.transpose(Ab, axes=[1, 0, 2]) # transpose the first 2 axes    \n",
    "    I = np.identity(n)\n",
    "    Ib = np.broadcast_to(np.expand_dims(A, axis=2), (n, n, m))\n",
    "    \n",
    "    dAdZ = Ab * (Ib - AbT)\n",
    "    \n",
    "    # dA - (n, m)\n",
    "    # dAdZ - (n, n, m)\n",
    "    dZ = np.einsum('ik, ijk -> jk',   dA , dAdZ)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with remapped shapes [original->remapped]: (2,4,1) and requested shape (2,2,4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-f07e089d7f89>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mcache\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0msoftmax_backward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-10-2d57ca55b032>\u001b[0m in \u001b[0;36msoftmax_backward\u001b[1;34m(dA, cache)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mAbT\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# transpose the first 2 axes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mI\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mIb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbroadcast_to\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mdAdZ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAb\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mIb\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mAbT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mbroadcast_to\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda\\miniconda\\lib\\site-packages\\numpy\\lib\\stride_tricks.py\u001b[0m in \u001b[0;36mbroadcast_to\u001b[1;34m(array, shape, subok)\u001b[0m\n\u001b[0;32m    180\u001b[0m            [1, 2, 3]])\n\u001b[0;32m    181\u001b[0m     \"\"\"\n\u001b[1;32m--> 182\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_broadcast_to\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubok\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubok\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda\\miniconda\\lib\\site-packages\\numpy\\lib\\stride_tricks.py\u001b[0m in \u001b[0;36m_broadcast_to\u001b[1;34m(array, shape, subok, readonly)\u001b[0m\n\u001b[0;32m    125\u001b[0m     it = np.nditer(\n\u001b[0;32m    126\u001b[0m         \u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'multi_index'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'refs_ok'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'zerosize_ok'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mextras\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m         op_flags=['readonly'], itershape=shape, order='C')\n\u001b[0m\u001b[0;32m    128\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mit\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m         \u001b[1;31m# never really has writebackifcopy semantics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with remapped shapes [original->remapped]: (2,4,1) and requested shape (2,2,4)"
     ]
    }
   ],
   "source": [
    "dA = np.random.randn(2, 4)\n",
    "cache = np.random.randn(2, 4)\n",
    "softmax_backward(dA, cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims):\n",
    "    \"\"\"\n",
    "    Initialize the parameters W's and b's.\n",
    "    \n",
    "    Argument:\n",
    "    layer_dims -- an array of layer dimensions [n0, n1, ..., nL]\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- a dictionary of initialized parameters\n",
    "                    W1: random\n",
    "                    b1: zeros\n",
    "                    ...\n",
    "                    WL: random\n",
    "                    bL: zeros\n",
    "    \"\"\"\n",
    "    parameters = {}\n",
    "    L = len(layer_dims) - 1\n",
    "    for l in range(0, L):\n",
    "        parameters['W'+str(l+1)] = np.random.randn(layer_dims[l+1], layer_dims[l]) * 0.01\n",
    "        parameters['b'+str(l+1)] = np.zeros((layer_dims[l+1], 1))\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test initialize_parameters()\n",
    "layer_dims = [5, 4, 1]\n",
    "parameters = initialize_parameters(layer_dims)\n",
    "print(parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear forward\n",
    "def linear_forward(A_prev, W, b):\n",
    "    \"\"\"\n",
    "    The linear part of forward propagation: Z = WA + b\n",
    "    \n",
    "    Argument:\n",
    "    A_prev -- the activitions of previous step (X for the first hidden layer)\n",
    "    W, b -- parameters \n",
    "    \n",
    "    Returns:\n",
    "    Z -- result of linear forward propagation\n",
    "    cache -- cache A_prev and the parameters W and b for convenience when calculating backpropagation\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = W @ A_prev + b\n",
    "    cache = (A_prev, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test linear forward\n",
    "layer_dims = [5, 4, 1]\n",
    "parameters = initialize_parameters(layer_dims)\n",
    "A_prev = np.array([1, 2, 3, 4, 5]).reshape((5, 1))\n",
    "W = parameters['W1']\n",
    "b = parameters['b1']\n",
    "Z, cache = linear_forward(A_prev, W, b)\n",
    "print('Z: ' + str(Z))\n",
    "print('cache: ' + str(cache))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Forward propagation including both linear mapping and activation\n",
    "    \n",
    "    Argument:\n",
    "    A_prev -- the activations of previous step (X for the first hidden layer)\n",
    "    W, b -- parameters\n",
    "    activation -- activation function used in this layer\n",
    "    \n",
    "    Returns:\n",
    "    A -- The activation values of current layer\n",
    "    cache -- A tuple containing both caches from linear part and activation part, (linear_cache, activation_cache)\n",
    "                where linear_cache is (A_prev, W, b), and activation_cache is Z\n",
    "    \"\"\"\n",
    "    \n",
    "    assert(W.shape[1]==A_prev.shape[0])\n",
    "    Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "    if activation == 'relu':        \n",
    "        A, activation_cache = relu(Z)\n",
    "    elif activation == 'sigmoid':\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    elif activation == 'softmax':\n",
    "        A, activation_cache = softmax(Z)\n",
    "    cache = (linear_cache, activation_cache)\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test linear_activation_forward\n",
    "layer_dims = [5, 4, 1]\n",
    "parameters = initialize_parameters(layer_dims)\n",
    "A_prev = np.array([1, 2, 3, 4, 5]).reshape((5, 1))\n",
    "W = parameters['W1']\n",
    "b = parameters['b1']\n",
    "A, cache = linear_activation_forward(A_prev, W, b, activation='softmax')\n",
    "print(\"A: \" + str(A))\n",
    "print(\"cache: \" + str(cache))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Use current parameters to calculate the output layer value AL.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data\n",
    "    parameters -- a dictionary containing W and b\n",
    "    \n",
    "    Returns:\n",
    "    AL -- output layer values \n",
    "    caches -- a list of cache, returned by linear_activation_forward() at each layer\n",
    "    \"\"\"\n",
    "    \n",
    "    caches = []\n",
    "    L = int(len(parameters) / 2)\n",
    "    A_prev = X\n",
    "    for l in range(1, L):\n",
    "        A_prev, cache = linear_activation_forward(A_prev, parameters['W'+str(l)], parameters['b'+str(l)], activation='relu')\n",
    "        caches.append(cache)\n",
    "    \n",
    "    # Last layer\n",
    "    AL, cache = linear_activation_forward(A_prev, parameters['W'+str(L)], parameters['b'+str(L)], activation='softmax')\n",
    "    caches.append(cache)\n",
    "    \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_dims = [5, 4, 3]\n",
    "parameters = initialize_parameters(layer_dims)\n",
    "X = np.array([1, 2, 3, 4, 5]).reshape((5, 1))\n",
    "AL, caches = L_model_forward(X, parameters)\n",
    "print('AL: ' + str(AL))\n",
    "print('caches: ' + str(len(caches)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compute cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Compute the cost function of softmax activation\n",
    "    \n",
    "    Argument:\n",
    "    AL -- hypothesis of the model\n",
    "    Y -- training labels\n",
    "    \n",
    "    Returns:\n",
    "    cost -- the value of cost function\n",
    "    \"\"\"\n",
    "    m = AL.shape[1]\n",
    "    cost = -1/m * np.sum(Y*np.log(AL))\n",
    "    cost = np.squeeze(cost)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_dims = [5, 4, 3]\n",
    "parameters = initialize_parameters(layer_dims)\n",
    "X = np.array([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]]).reshape((5, 2))\n",
    "Y = np.array([1, 0, 0, 0, 0, 1]).reshape((3, 2))\n",
    "AL, caches = L_model_forward(X, parameters)\n",
    "cost = compute_cost(AL, Y)\n",
    "print('cost: ' + str(cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Backward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear part back propagation:**\n",
    "\n",
    "Convert the derivative of cost function with respect to $Z$, the neuron value before activation, $\\frac{\\partial L}{\\partial Z}$, to the derivative with respect to parameters $W$ and $b$, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Linear part of the backward propagation: \n",
    "    dW = 1/m * dZ * dA_prev.T (n[l], n[l-1]) (n[l], m) (n[l-1], m)\n",
    "    db = 1/m * np.sum(dZ, axis=1, keepdims=True)\n",
    "    \n",
    "    Args:\n",
    "    dZ -- derivative of cost function with respect to Z\n",
    "    cache -- cache from linear forward propagation, (A, W, b)\n",
    "    \n",
    "    Returns:\n",
    "    dW, db -- gradients used for gradient descent\n",
    "    \"\"\"\n",
    "    \n",
    "    m = dZ.shape[1]\n",
    "    A_prev = cache[0]\n",
    "    \n",
    "    dW = 1/m * dZ @ A_prev.T\n",
    "    db = 1/m * np.sum(dZ, axis=1, keepdims=True)\n",
    "    \n",
    "    \n",
    "    return dW, db "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dZ = np.array([1, 2, 3]).reshape((3, 1))\n",
    "cache = (np.random.randn(4, 1), 0, 0)\n",
    "dW, db = linear_backward(dZ, cache)\n",
    "print('dW: ' + str(dW))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Linear + activation parts, whole backward propagation dA -> dA_prev\n",
    "    \n",
    "    Args:\n",
    "    dA -- derivative of cost function with respect to A\n",
    "    cache -- cache from linear activation forward propagation, (linear_cache, activation_cache)\n",
    "    \n",
    "    Returns:\n",
    "    dW, db -- gradients used for gradient descent\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    W = linear_cache[1]\n",
    "    \n",
    "    if activation == 'sigmoid':\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "    elif activation == 'relu':\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "    elif activation == 'softmax':\n",
    "        dZ = softmax_backward(dA, activation_cache)\n",
    "    \n",
    "    dA_prev = W.T @ dZ\n",
    "    \n",
    "    dW, db = linear_backward(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dA = np.array([1, 2, 3]).reshape((3, 1))\n",
    "cache = ((np.random.randn(4, 1), np.random.randn(3, 4), 0), np.random.randn(3, 1))\n",
    "dA_prev, dW, db = linear_activation_backward(dZ, cache, 'relu')\n",
    "print('dW: ' + str(dW))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    AL -- activation values of the last layer\n",
    "    Y -- data labels\n",
    "    caches -- a list of caches from each layer, layer one cache corresponds to caches[0]\n",
    "    \n",
    "    Returns:\n",
    "    grads -- a dict of gradients (of loss function wrt parameters) in each layer, will be used to update the parameters.\n",
    "    \"\"\"\n",
    "    \n",
    "    grads = {}\n",
    "    dAL = -(Y / AL)\n",
    "    m = Y.shape[1]\n",
    "    L = len(caches)\n",
    "    \n",
    "    cache = caches[L-1]\n",
    "    \n",
    "    grads['dA'+str(L-1)], grads['dW'+str(L)], grads['db'+str(L)] = linear_activation_backward(dAL, cache, 'sigmoid')    \n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        grads['dA'+str(l)], grads['dW'+str(l+1)], grads['db'+str(l+1)] = linear_activation_backward(grads['dA'+str(l+1)], caches[l], 'relu')\n",
    "        \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inverse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Update parameters\n",
    "\n",
    "Update parameters W and b using the grads value returned by ```linear_activation_backward()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    parameters -- current parameter W and b\n",
    "    grads -- gradients of loss function wrt W and b \n",
    "    learning_rate -- rate of gradient descent\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- updated W and b\n",
    "    \"\"\"\n",
    "#     print('len parameters: ' + str(len(parameters)))\n",
    "#     print('len grads: ' + str(len(grads)))\n",
    "#     assert(len(parameters)==len(grads))\n",
    "    L = len(parameters) // 2\n",
    "    \n",
    "    for l in range(L):    \n",
    "        parameters['W' + str(l+1)] -= learning_rate * grads['dW' + str(l+1)]\n",
    "        parameters['b' + str(l+1)] -= learning_rate * grads['db' + str(l+1)]\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test update_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = x_train.shape[0]\n",
    "r, c = x_train.shape[1], x_train.shape[1]\n",
    "X = x_train.reshape((m, r*c))\n",
    "X = X.T / 255\n",
    "Y = y_train.reshape((1, m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_dims = [r*c, 20, 10]\n",
    "parameters = initialize_parameters(layer_dims)\n",
    "AL, caches = L_model_forward(X, parameters)\n",
    "grads = L_model_backward(AL, Y, caches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Iterate the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, layer_dims, learning_rate, num_iterations=1000, print_cost=False):\n",
    "    L = len(layer_dims) - 1\n",
    "    n0, m = X.shape\n",
    "    parameters = initialize_parameters(layer_dims)\n",
    "    for i in range(num_iterations):\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "        print('finish forward')\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    "        print('finish backward')\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        print('parameters updated')\n",
    "        cost = compute_cost(AL, Y)\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print('Iteration {0} cost: {1}'.format(i, cost))\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 cost: 592.5887621694845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\miniconda\\lib\\site-packages\\ipykernel_launcher.py:13: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  del sys.path[0]\n",
      "E:\\anaconda\\miniconda\\lib\\site-packages\\ipykernel_launcher.py:13: RuntimeWarning: invalid value encountered in true_divide\n",
      "  del sys.path[0]\n",
      "E:\\anaconda\\miniconda\\lib\\site-packages\\ipykernel_launcher.py:17: RuntimeWarning: invalid value encountered in multiply\n",
      "E:\\anaconda\\miniconda\\lib\\site-packages\\ipykernel_launcher.py:13: RuntimeWarning: divide by zero encountered in log\n",
      "  del sys.path[0]\n",
      "E:\\anaconda\\miniconda\\lib\\site-packages\\ipykernel_launcher.py:13: RuntimeWarning: invalid value encountered in multiply\n",
      "  del sys.path[0]\n",
      "E:\\anaconda\\miniconda\\lib\\site-packages\\ipykernel_launcher.py:15: RuntimeWarning: invalid value encountered in greater_equal\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 100 cost: nan\n",
      "Iteration 200 cost: nan\n",
      "Iteration 300 cost: nan\n",
      "Iteration 400 cost: nan\n",
      "Iteration 500 cost: nan\n",
      "Iteration 600 cost: nan\n",
      "Iteration 700 cost: nan\n",
      "Iteration 800 cost: nan\n",
      "Iteration 900 cost: nan\n"
     ]
    }
   ],
   "source": [
    "layer_dims = [r*c, 50, 10]\n",
    "parameters = L_layer_model(X, Y, layer_dims, learning_rate=0.01, num_iterations=1000, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Numerical instability**\n",
    "When iterating softmax classification, we encountered numerical instability where np.exp overflows. This happens typically because the Z values sometimes get too large. To avoid this, we look at the formula for computing softmax again:\n",
    "\n",
    "$$ S_j = \\frac{\\exp z_j}{\\sum_i \\exp z_i}$$\n",
    "\n",
    "if we multiply the numerator and denominator both with a constant real number C, the formula becomes\n",
    "\n",
    "$$ S_j = \\frac{C\\exp z_j}{\\sum_i C\\exp z_i}  = \\frac{\\exp z_j+\\log C}{\\sum_i \\exp z_i + \\log C}$$\n",
    "\n",
    "The overflow of $\\exp z_j$ is cause by large $z_j$. We see in the above equation that by adding a same constant to all $z$'s does not influence the output value of softmax function. Therefore, we modify the softmax function as follows: \n",
    "\n",
    "$$ S_j = \\frac{\\exp z_j - \\max(Z)}{\\sum_i \\exp z_i - \\max(Z)}$$\n",
    "\n",
    "This turns out not to be very helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x23da753bb48>"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X[:, 2].reshape((r, c)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[200, 2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
