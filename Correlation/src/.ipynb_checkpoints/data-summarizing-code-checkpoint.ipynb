{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data summarizing code\n",
    "\n",
    "Save the code snipets that summarize data obtained directly from batch processes such as *df2_kinetics.py*. The snipets here should generate a *summary.csv* file for each day's data (of a specific type). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guidelines\n",
    "- Each snipet should contain a summarizing part and a retrieval part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'> Plans\n",
    "\n",
    "<font color='blue'>\n",
    "- ...\n",
    "    \n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "1. Correlation length: angle and velocity\n",
    "2. df2_kinetics\n",
    "    - GNF curves at different concentrations\n",
    "    - $\\alpha$\n",
    "    - Order parameter\n",
    "3. Summarize local correlation between df and energy\n",
    "4. Summarize energy spectrum data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from corrLib import readdata\n",
    "from corr_utils import *\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Correlation length: angle and velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize data\n",
    "# sample 10 frames out of the last 1/3 of frames\n",
    "corr_data_path = r'E:\\moreData\\08052020\\cav_imseq'\n",
    "num_sample = 20 # number of frames to sample\n",
    "sample_clA = []\n",
    "sample_clV = []\n",
    "sfL = next(os.walk(corr_data_path))[1]\n",
    "sfL_new = []\n",
    "for s in sfL:\n",
    "#     print('sample ' + str(n))\n",
    "    corr_data_path_num = os.path.join(corr_data_path, s)\n",
    "    \n",
    "    # sample 20 frames in the last 1/3 frames\n",
    "    \n",
    "    l = len(corrLib.readdata(corr_data_path_num))\n",
    "    if l > 60:\n",
    "        samples = np.random.randint((l*2/3)/2, (l-1)/2, num_sample) * 2\n",
    "        clAL = []\n",
    "        clVL = []\n",
    "        for i in samples:\n",
    "            data_raw = pd.read_csv(os.path.join(corr_data_path_num, '{0:04d}-{1:04d}.csv'.format(i, i+1))) # X Y CA CV\n",
    "            data_AV = xy_to_r(data_raw).sort_values(by=['R'])     \n",
    "            clA, fit = corr_length(data_AV.rename(columns={'CA': 'C'}), fitting_range=500)\n",
    "            clV, fit = corr_length(data_AV.rename(columns={'CV': 'C'}), fitting_range=500)\n",
    "            clAL.append(clA)\n",
    "            clVL.append(clV)\n",
    "        sample_clA.append(np.array(clAL).mean())\n",
    "        sample_clV.append(np.array(clVL).mean())\n",
    "        sfL_new.append(int(s))\n",
    "    else:\n",
    "        raise ValueError('Too few data to sample from. Need at least 60 .csv files.')\n",
    "summary = pd.DataFrame({'sample': sfL_new, 'clA': sample_clA, 'clV': sample_clV})\n",
    "summary.to_csv(os.path.join(corr_data_path, 'summary.csv'), index=False)\n",
    "### Needs further improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve data from cav_imseq/summary.csv\n",
    "folder = r'E:\\moreData'\n",
    "subfolder_name = 'cav_imseq'\n",
    "data = {'conc': [], 'clA_avg': [], 'clV_avg': [], 'clA_std': [], 'clV_std': []}\n",
    "for kw in dirs:\n",
    "    if kw != '00':\n",
    "        conc = int(kw)\n",
    "        for n, dn in enumerate(dirs[kw]):\n",
    "            date, num = dn.split('-')\n",
    "            summary_dir = os.path.join(folder, date, subfolder_name, 'summary.csv')\n",
    "            temp = pd.read_csv(summary_dir, index_col='sample').loc[[int(num)]]\n",
    "            if n == 0:\n",
    "                conc_data = temp\n",
    "            else:\n",
    "                conc_data = conc_data.append(temp)\n",
    "        data['conc'].append(conc)\n",
    "        data['clA_avg'].append(conc_data.clA.mean())\n",
    "        data['clV_avg'].append(conc_data.clV.mean())\n",
    "        data['clA_std'].append(conc_data.clA.std())\n",
    "        data['clV_std'].append(conc_data.clV.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 df2_kinetics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 GNF curves at different concentrations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = []\n",
    "num = []\n",
    "conc = []\n",
    "for kw in dirs:\n",
    "    if int(kw) <= 120:\n",
    "        conc.append(int(kw))\n",
    "        d, n = dirs[kw][1].split('-')\n",
    "        date.append(d)\n",
    "        num.append(int(n))\n",
    "data_log = pd.DataFrame().assign(conc=conc, date=date, num=num).sort_values(by='conc')\n",
    "# NOTE: \n",
    "# This block is not elegant. I want to use a consistent method to load experiment log information and map them to data.\n",
    "# Throughout this project, I have implemented no less than three of such methods. Yet, none of them are intuitive or easy to use.\n",
    "# Keep this block here for now, change it in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = 10\n",
    "data_dir = r'E:\\moreData\\{0}\\df2_kinetics\\{1:02d}\\kinetics_data.csv'\n",
    "count = 0\n",
    "for num, i in data_log.iterrows():\n",
    "    if i.conc != 0 and i.conc != 85:# and i.conc < 40:     # for animation in slides   \n",
    "        k_data = pd.read_csv(data_dir.format(i.date, i.num))\n",
    "        gnf_data = k_data.loc[k_data.segment==k_data.segment.max()]\n",
    "        x, y = postprocess_gnf(gnf_data, lb, xlim=[10, 10000], sparse=1)\n",
    "#         ax.plot(x, y, marker=marker_list[color_dict[str(i.conc)]], label='{:.1f}'.format(i.conc*0.08), color=wowcolor(color_dict[str(i.conc)]), markersize=2, lw=1)\n",
    "        temp = pd.concat([x, y], axis=1, keys=['N', str(i.conc)]).set_index('N')\n",
    "        if count == 0:\n",
    "            master = temp\n",
    "        else:\n",
    "            master = pd.concat([master, temp], axis=1)\n",
    "        count += 1\n",
    "master.to_csv(r'E:\\Google Drive\\Research projects\\DF\\data\\gnf.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 $\\alpha$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize data from each raw data folders\n",
    "dates = ['08032020', '08042020', '08052020', '08062020']\n",
    "sampling_range = 1 # range between 0 and 1, above which we sample the alpha values and average\n",
    "fitting_range = [10, ] # um\n",
    "############################################\n",
    "\n",
    "for date in dates:\n",
    "    date_folder = r'D:\\density_fluctuations\\{}'.format(date)\n",
    "    k_master_folder = os.path.join(date_folder, 'df2_kinetics')\n",
    "    i_master_folder = os.path.join(date_folder, 'overall_intensity')\n",
    "    sfL = next(os.walk(k_master_folder))[1]\n",
    "    alpha_list = []\n",
    "    n_list = []\n",
    "    for sf in sfL:\n",
    "        n = int(sf)\n",
    "        fps = data_log()[date]['fps'][n]\n",
    "        k_folder = r'D:\\density_fluctuations\\{0}\\df2_kinetics\\{1:02d}'.format(date, n)\n",
    "        i_folder = r'D:\\density_fluctuations\\{0}\\overall_intensity\\{1:02d}'.format(date, n)\n",
    "        k_data = pd.read_csv(os.path.join(k_folder, 'kinetics_data.csv'))\n",
    "        i_data = pd.read_csv(os.path.join(i_folder, 'intensity.csv'))\n",
    "        data = plot_kinetics(k_data, i_data, tlim=None, xlim=[5, 100], lb=10, mpp=0.33, seg_length=50, fps=fps, plot=False)\n",
    "        df = pd.DataFrame().assign(t=data['t0'], alpha=data['alpha'])\n",
    "        alpha = df['alpha'].loc[df['t']>=df['t'].max()*sampling_range].mean()\n",
    "        n_list.append(n)\n",
    "        alpha_list.append(alpha)\n",
    "    data = pd.DataFrame({'n': n_list, 'alpha': alpha_list})\n",
    "    data.to_csv(os.path.join(k_master_folder, 'summary.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Order parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eo_folder = r'D:\\density_fluctuations\\08042020\\energy_order'\n",
    "sfL = next(os.walk(eo_folder))[1]\n",
    "OP_list = []\n",
    "E_list = []\n",
    "for sf in sfL:\n",
    "    print('summarizing data in {}'.format(sf))\n",
    "    eo_data = pd.read_csv(os.path.join(eo_folder, sf, 'energy_order.csv'))\n",
    "    eo_data_crop = eo_data.loc[eo_data.t>eo_data.t.max()*0.9]\n",
    "    E = eo_data_crop.E.mean()\n",
    "    E_list.append(E)\n",
    "    OP = eo_data_crop.OP.mean()\n",
    "    OP_list.append(OP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame().assign(sample=sfL, E=E_list, OP=OP_list)\n",
    "data.to_csv(os.path.join(eo_folder, 'eo_summary.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 summarize local correlation between df and energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = r'D:\\density_fluctuations\\08062020\\local_df'\n",
    "sfL = next(os.walk(folder))[1]\n",
    "avg_list = []\n",
    "for sf in sfL:\n",
    "    print('summarizing data in {}'.format(sf))\n",
    "    df_folder = os.path.join(folder, sf, 'dt=10')\n",
    "    piv_folder = r'D:\\density_fluctuations\\08062020\\piv_imseq\\{}'.format(sf)\n",
    "    l = readdata(df_folder, 'npy')\n",
    "    l = l.loc[l.Name.astype('int')>l.Name.astype('int').max()*0.8]\n",
    "    corr_list = []\n",
    "    for num, i in l.iterrows():\n",
    "        df = np.load(i.Dir)\n",
    "        n = int(i.Name)\n",
    "        pivData = pd.read_csv(os.path.join(piv_folder, '{0:04d}-{1:04d}.csv'.format(n, n+1)))\n",
    "        v = np.array(pivData.u**2 + pivData.v**2).reshape((42, 50))\n",
    "        corr = corr2d(df, v)\n",
    "        corr_list.append(corr)\n",
    "    avg_corr = np.array(corr_list).mean()\n",
    "    avg_list.append(avg_corr)\n",
    "    print('{0}: {1:.3f}'.format(sf, avg_corr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame().assign(sample=sfL, E=avg_list)\n",
    "data.to_csv(os.path.join(folder, 'corr_E_sl=10.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Summarize energy spectrum data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_es(folder):\n",
    "    \"\"\"\n",
    "    Average energy spectrum data in each subfolder under folder, then save a summary.csv file under folder, containing all the averaged data.\n",
    "    \n",
    "    \"\"\"\n",
    "    sfL = next(os.walk(folder))[1]\n",
    "    count = 0\n",
    "    for sf in sfL:\n",
    "        clear_output(wait=True)\n",
    "        print('Summarizing {0}\\\\{1} ...'.format(folder, sf))\n",
    "        es_folder = os.path.join(folder, sf) # for varstep, experiment number '00' needs to be included\n",
    "        l = readdata(es_folder, 'csv')\n",
    "        for num, i in l.iterrows():\n",
    "            if num == 0:\n",
    "                data = pd.read_csv(i.Dir)\n",
    "            else:\n",
    "                data += pd.read_csv(i.Dir)\n",
    "        data /= num + 1\n",
    "        if count == 0:\n",
    "            summary = data.assign(sample=sf)\n",
    "        else:\n",
    "            summary = summary.append(data.assign(sample=sf))\n",
    "        count += 1\n",
    "    summary.to_csv(os.path.join(folder, 'summary.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing E:\\moreData\\08062020\\energy_spectrum\\03 ...\n"
     ]
    }
   ],
   "source": [
    "folders = ['08032020', '08042020', '08052020', '08062020']\n",
    "for f in folders:\n",
    "    folder = os.path.join(r'E:\\moreData', f, 'energy_spectrum')\n",
    "    summarize_es(folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
